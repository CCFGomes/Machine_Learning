{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5210358,"sourceType":"datasetVersion","datasetId":3030717}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\nWine, an age-old beverage, is produced through the fermentation of grapes and comes in various types, most notably red and white. The differentiation between these types is primarily due to the grape varieties used and the winemaking process, particularly the fermentation method (Jackson, 2020). Red wines are fermented with grape skins, imparting their characteristic color and tannins, while white wines are typically fermented without skins, resulting in a lighter color and flavor profile (Rib√©reau-Gayon et al., 2006).\n\nThe quality of wine is influenced by a myriad of physicochemical properties such as acidity, sugar level, alcohol content, sulphates, and pH level. These attributes significantly impact the taste, aroma, and overall sensory experience of the wine (Cortez et al., 2009). Understanding these factors and their variations is crucial for winemakers aiming to produce high-quality wines that meet consumer preferences.\n\n## **Objectives**\n\nThis project aims to achieve three primary objectives:\n\n1. **To determine the essential physicochemical properties that affect the quality of wine:** By analyzing various physicochemical attributes, we aim to identify which factors most significantly influence wine quality.\n2. **To build regression models for predicting wine quality:** The objective is to create and optimize regression models that can accurately predict the numerical quality score of wines using their physicochemical attributes.\n3. **To build classification models for predicting wine type:** The objective is to create classification models that can reliably identify red and white wines based on quantitative properties, including acidity, alcohol content, and sugar levels.\n\n## **Purpose of Predictive Models**\n\nThe purpose of the predictive models is to provide winemakers and industry professionals with tools to evaluate and predict wine quality based on its physicochemical properties. This can aid in quality control, product development, and market positioning by allowing for the assessment of wine quality before it reaches consumers.\n\n## **Steps to Accomplish Objectives**\n\n1. **Data Collection and Preprocessing:**\n   - Combine winequality-red and winequality-white datasets.\n   - Clean the data and create a binary column (`type_bin`) to differentiate red (1) and white (0) wines.\n\n2. **Data Visualization and Exploratory Data Analysis (EDA):**\n   - **Histograms:** Created for various physicochemical variables like fixed acidity, volatile acidity, citric acid, etc., with differentiation between red and white wine types.\n   - **Bar Charts:** Illustrate the distribution of wine quality across different wine types.\n   - **Grouped Bar Plots:** Comparing different chemical attributes that determine wine taste, such as fixed acidity, alcohol, and residual sugar.\n   - **Box Plot:** Focus on the distribution of citric acid across different wine types.\n   - **Scatter Plot:** Analyze the relationship between selected pairs of variables.\n   - **Correlation Heatmap:** Visualize the correlation matrix to identify relationships between various physicochemical properties.\n\n3. **Feature Importance Analysis:**\n   - Use Random Forest to determine the importance of physicochemical properties affecting wine quality.\n\n4. **Regression Modeling for Quality Prediction:**\n   - **Random Forest Regression:** Train and evaluate the model, and visualize feature importance.\n   - **XGBoost Regression:** Train and optimize the model, and evaluate using metrics like Mean Squared Error (MSE), Mean Absolute Error (MAE), and Root Mean Squared Error (RMSE).\n   - **Decision Tree Regression:** Train the model, visualize the decision tree, and evaluate its performance.\n\n5. **Classification Modeling for Wine Type Prediction:**\n   - **Support Vector Machine (SVM):** Train and evaluate the model using metrics like precision, recall, and F1 score.\n   - **k-Nearest Neighbors (k-NN):** Train and evaluate the model using the same metrics.\n   - **Comparative Analysis:** Compare the performance of Decision Tree, SVM, and k-NN models.\n\n6. **Model Evaluation and Comparison:**\n   - Create a data frame with the performance metrics of all models.\n   - Generate a comparison table to analyze the effectiveness of each model.\n\n## **Libraries Used**\n\nTo implement the steps above, we will use the following Python libraries:\n\n- **Pandas:** For data manipulation and preprocessing.\n- **NumPy:** For numerical computations.\n- **Matplotlib and Seaborn:** For data visualization.\n- **Scikit-learn:** For machine learning models and evaluation.\n- **XGBoost:** For gradient boosting models.\n- **SciPy:** For statistical analysis.\n\nThis comprehensive analysis will not only identify key physicochemical properties influencing wine quality but also develop robust predictive models for wine quality and type classification. The insights gained from this study can significantly benefit winemakers in quality control and product development, ultimately enhancing the wine production process.","metadata":{}},{"cell_type":"markdown","source":"# 1 Data Preparation","metadata":{}},{"cell_type":"code","source":"# Import Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.metrics import confusion_matrix, classification_report, mean_squared_error, mean_absolute_error, accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.tree import DecisionTreeRegressor, plot_tree\nfrom sklearn import svm\nfrom sklearn.neighbors import KNeighborsClassifier\nimport xgboost as xgb\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Listing Files in a Directory\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input/wine-quality-data'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Load the Data\nred_wine = pd.read_csv('/kaggle/input/wine-quality-data/winequality-red.csv', sep=';')\nwhite_wine = pd.read_csv('/kaggle/input/wine-quality-data/winequality-white.csv', sep=';')\n\n# Add Type Bin Columns\nred_wine['type_bin'] = 1\nwhite_wine['type_bin'] = 0\n\n# Combine data\nwine_data = pd.concat([red_wine, white_wine], axis=0).reset_index(drop=True)\n\n# Display the first few rows of each dataset\nprint(\"Red Wine Data:\")\nprint(red_wine.head())\nprint(\"\\nWhite Wine Data:\")\nprint(white_wine.head())","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-04T23:40:06.293745Z","iopub.execute_input":"2024-08-04T23:40:06.294594Z","iopub.status.idle":"2024-08-04T23:40:09.425844Z","shell.execute_reply.started":"2024-08-04T23:40:06.294554Z","shell.execute_reply":"2024-08-04T23:40:09.424579Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2 Exploratory Data Analysis EDA","metadata":{}},{"cell_type":"code","source":"# Check for missing values\nprint(\"Red Wine Missing Values:\")\nprint(red_wine.isnull().sum())\nprint(\"\\nWhite Wine Missing Values:\")\nprint(white_wine.isnull().sum())\n\n# Get summary statistics\nprint(\"Red Wine Summary Statistics:\")\nprint(red_wine.describe())\nprint(\"\\nWhite Wine Summary Statistics:\")\nprint(white_wine.describe())\n\n# Check unique values in quality for both datasets\nprint(\"Red Wine Quality Values:\")\nprint(red_wine['quality'].unique())\n\nprint(\"White Wine Quality Values:\")\nprint(white_wine['quality'].unique())\n","metadata":{"execution":{"iopub.status.busy":"2024-08-04T23:44:17.591613Z","iopub.execute_input":"2024-08-04T23:44:17.592035Z","iopub.status.idle":"2024-08-04T23:44:17.676919Z","shell.execute_reply.started":"2024-08-04T23:44:17.592004Z","shell.execute_reply":"2024-08-04T23:44:17.675811Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Missing Values and Summary Statistics:\n\nNo missing values were found. \n\nThe Summary Statistics reveals key differences between red and white wines in terms of their chemical attributes, with red wines generally having higher acidity and lower residual sugar compared to white wines. \nThe feature importance analysis can help identify which attributes are most influential in determining wine quality.","metadata":{}},{"cell_type":"markdown","source":"### Histograms","metadata":{}},{"cell_type":"code","source":"# Histograms \nfeatures = wine_data.columns[:-2]  # Exclude 'quality' and 'type_bin'\nfor feature in features:\n    plt.figure(figsize=(10, 6))\n    sns.histplot(data=wine_data, x=feature, hue='type_bin', kde=True)\n    plt.title(f'Histogram of {feature}')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-04T23:44:23.632273Z","iopub.execute_input":"2024-08-04T23:44:23.633359Z","iopub.status.idle":"2024-08-04T23:44:33.382868Z","shell.execute_reply.started":"2024-08-04T23:44:23.633310Z","shell.execute_reply":"2024-08-04T23:44:33.381709Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Histograms:** The histograms provide a visual distribution of each feature across red and white wines. For instance:\n\n* Fixed Acidity: Red wines generally have higher fixed acidity, contributing to their more robust flavor (Jackson, 2014).\n* Alcohol: White wines often have higher alcohol content, which can affect sweetness and body (Robinson, 2019).\n* Residual Sugar: White wines exhibit a wider range of residual sugar, often higher than in red wines.\n\nThese histograms help identify the differences in the distribution of key attributes between the two types of wine, which may impact their quality and sensory profile.","metadata":{}},{"cell_type":"markdown","source":"### Bar Chart of Wine Quality by Type","metadata":{}},{"cell_type":"code","source":"# Bar chart of wine quality by type\nplt.figure(figsize=(10, 6))\nsns.countplot(x='quality', hue='type_bin', data=wine_data)\nplt.title('Wine Quality Distribution by Type')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-04T23:44:33.384576Z","iopub.execute_input":"2024-08-04T23:44:33.384876Z","iopub.status.idle":"2024-08-04T23:44:33.714264Z","shell.execute_reply.started":"2024-08-04T23:44:33.384851Z","shell.execute_reply":"2024-08-04T23:44:33.713213Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Quality Distribution:\n\n* Red Wine: Quality ratings range from 3 to 8, with a concentration around 5 and 6.\n\n* White Wine: Quality ratings range from 3 to 9, with a concentration around 5, 6, and 7.\n\nThe distribution of wine quality reveals that both red and white wines have a mix of quality ratings, with the majority being clustered around the mid-range values.","metadata":{}},{"cell_type":"markdown","source":"### Grouped Bar Plot of Selected Features\n","metadata":{}},{"cell_type":"code","source":"# Grouped bar plot\ngroup_features = ['fixed acidity', 'alcohol', 'residual sugar']\nfor feature in group_features:\n    plt.figure(figsize=(10, 6))\n    sns.barplot(x='type_bin', y=feature, data=wine_data)\n    plt.title(f'{feature} by Wine Type')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-04T23:44:38.267774Z","iopub.execute_input":"2024-08-04T23:44:38.268296Z","iopub.status.idle":"2024-08-04T23:44:39.345825Z","shell.execute_reply.started":"2024-08-04T23:44:38.268254Z","shell.execute_reply":"2024-08-04T23:44:39.344724Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Fixed Acidity: Higher in red wines, contributing to their flavor profile (Jackson, 2014).\n* Alcohol: Higher in white wines, affecting sweetness and body (Robinson, 2019).\n* Residual Sugar: Generally lower in red wines, influencing taste (Jackson, 2014).\n\nThese features are selected due to their significant influence on wine quality and sensory attributes. The grouped bar plot helps visualize these differences.","metadata":{}},{"cell_type":"code","source":"# Box-plot for citric acid\nplt.figure(figsize=(10, 6))\nsns.boxplot(x='type_bin', y='citric acid', data=wine_data)\nplt.title('Distribution of Citric Acid by Wine Type')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-04T23:44:46.993359Z","iopub.execute_input":"2024-08-04T23:44:46.994073Z","iopub.status.idle":"2024-08-04T23:44:47.274604Z","shell.execute_reply.started":"2024-08-04T23:44:46.994022Z","shell.execute_reply":"2024-08-04T23:44:47.273501Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Box Plot**\n\nThe box plot shows the distribution of citric acid, which affects the wine‚Äôs acidity and freshness. Citric acid can enhance the balance of flavors, contributing to the overall quality of the wine (Jackson, 2014). The plot indicates that citric acid levels vary between red and white wines, which can impact the sensory profile.","metadata":{}},{"cell_type":"code","source":"# Scatter plots \nscatter_features = [('fixed acidity', 'pH'), ('alcohol', 'density')]\nfor x_feature, y_feature in scatter_features:\n    plt.figure(figsize=(10, 6))\n    sns.scatterplot(x=x_feature, y=y_feature, hue='type_bin', data=wine_data)\n    plt.title(f'{x_feature} vs {y_feature}')\n    plt.show()\n    ","metadata":{"execution":{"iopub.status.busy":"2024-08-04T23:44:50.564830Z","iopub.execute_input":"2024-08-04T23:44:50.565252Z","iopub.status.idle":"2024-08-04T23:44:51.783721Z","shell.execute_reply.started":"2024-08-04T23:44:50.565219Z","shell.execute_reply":"2024-08-04T23:44:51.782442Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"    # Extract data used for plotting\n    scatter_data = wine_data[[x_feature, y_feature, 'type_bin']]\n    \n    # Print data for analysis\n    print(f\"Data for {x_feature} vs {y_feature}:\")\n    print(scatter_data.head())  # Print first few rows for a preview\n    # If you want to save this data to a file for further analysis:\n    scatter_data.to_csv(f\"{x_feature}_vs_{y_feature}_data.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-08-05T00:58:33.933434Z","iopub.execute_input":"2024-08-05T00:58:33.934024Z","iopub.status.idle":"2024-08-05T00:58:33.974237Z","shell.execute_reply.started":"2024-08-05T00:58:33.933987Z","shell.execute_reply":"2024-08-05T00:58:33.973129Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Fixed Acidity vs. pH: The scatter plot shows a negative correlation between fixed acidity and pH, indicating that as the fixed acidity of the wine increases, its pH value decreases. This suggests that wines with higher acidity have lower pH levels, making them more acidic. This relationship is important because pH is a key factor in determining the taste and preservation qualities of the wine. A lower pH usually corresponds to a more acidic taste, which is often desired in certain wine styles (Robinson, 2019).\n\n* Alcohol vs. Density: The scatter plot reveals a negative correlation between alcohol and density. This implies that as the alcohol content increases, the density of the wine tends to decrease. The underlying reason for this trend is that alcohol is less dense than water; thus, higher alcohol concentrations reduce the overall density of the wine. This information is valuable for understanding how alcohol levels impact the physical properties of the wine, which can affect its mouthfeel and overall quality (Jackson, 2014).\n\nThese relationships are crucial for understanding how certain attributes interact and affect wine quality.","metadata":{}},{"cell_type":"code","source":"# Correlation Heatmap\nplt.figure(figsize=(12, 8))\nsns.heatmap(wine_data.corr(), annot=True, cmap='coolwarm')\nplt.title('Correlation Heatmap of Wine Attributes')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-05T00:25:40.437494Z","iopub.execute_input":"2024-08-05T00:25:40.437932Z","iopub.status.idle":"2024-08-05T00:25:41.456588Z","shell.execute_reply.started":"2024-08-05T00:25:40.437897Z","shell.execute_reply":"2024-08-05T00:25:41.455135Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Correlation Analysis Insights\n\n####  **Key Correlations:**\n\n**1. Density and Alcohol (-0.687)**\n\nThere is a strong negative correlation between `Density` and `Alcohol`. This indicates that as the alcohol content in the wine increases, its density tends to decrease. This is consistent with the fact that alcohol is less dense than water and contributes less to the overall density of the wine. As the alcohol content increases, the relative proportion of denser substances (like sugars) decreases, leading to a lower density overall (Ough & Amerine, 1988).\n\n\n**2. Chlorides and Volatile Acidity (0.377)**\n\nThere is a moderate positive correlation between `Chlorides` and `Volatile Acidity`. This suggests that wines with higher chloride levels tend to have higher volatile acidity. This could be due to the impact of chlorides on the stability of the wine and its tendency to develop higher levels of volatile acids during fermentation or aging (Jackson, 2008).\n\n\n**3. Residual Sugar and Total Sulfur Dioxide (0.496)**\n\nA moderate positive correlation between `Residual Sugar` and `Total Sulfur Dioxide` indicates that wines with higher residual sugar levels often have higher total sulfur dioxide. This correlation likely reflects the need for higher sulfur dioxide levels to prevent spoilage in sweeter wines, which are more susceptible to microbial activity (Boulton et al., 1996).\n\n\n**4. Volatile Acidity and Fixed Acidity (0.219)**\n\nThere is a weak positive correlation between `Volatile Acidity` and `Fixed Acidity`. This suggests that as volatile acidity increases, fixed acidity tends to increase slightly. This can be due to the fact that both types of acidity are components of the wine's overall acidity profile, though they are influenced by different factors (Jackson, 2008).\n\n\n**5. Residual Sugar and Density (0.553)**\n\nA moderate positive correlation between `Residual Sugar` and `Density` means that higher levels of residual sugar are associated with higher wine density. This is because sugars contribute to the overall weight of the wine, increasing its density (Ough & Amerine, 1988).\n\n\n**6. Volatile Acidity and Fixed Acidity (0.219)**\n\nA weak positive correlation between `Volatile Acidity` and `Fixed Acidity` indicates that wines with higher volatile acidity tend to have slightly higher fixed acidity. This can reflect the overall acidity balance in the wine, though the relationship is not strong (Jackson, 2008).\n\n\n**7. Sulphates and Chlorides (0.396)**\n\nThere is a weak to moderate positive correlation between `Sulphates` and `Chlorides`. This suggests that higher sulfate levels in wine are somewhat associated with higher chloride levels. The connection might be due to the overall mineral content of the wine, although the correlation is not very strong (Boulton et al., 1996).\n\n\n**8. Alcohol and Residual Sugar (-0.359)**\n\nThere is a moderate negative correlation between `Alcohol` and `Residual Sugar`. This means that as residual sugar levels increase, alcohol content tends to decrease. This can be explained by the fermentation process where higher sugar levels are often associated with lower final alcohol content due to incomplete fermentation (Ough & Amerine, 1988).\n\nThese correlations provide a clearer understanding of how different chemical properties interact in wine, which can be valuable for quality control and wine production processes.","metadata":{}},{"cell_type":"code","source":"# Create Quality-type variable\nwine_data['qualitytype'] = wine_data['quality'].apply(lambda q: 'good' if q >= 6 else 'bad')\n\n# Split data into train and test sets\ntrain, test = train_test_split(wine_data, test_size=0.3, random_state=123)","metadata":{"execution":{"iopub.status.busy":"2024-08-04T23:45:02.465234Z","iopub.execute_input":"2024-08-04T23:45:02.465638Z","iopub.status.idle":"2024-08-04T23:45:02.480195Z","shell.execute_reply.started":"2024-08-04T23:45:02.465607Z","shell.execute_reply":"2024-08-04T23:45:02.479108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3 Machine Learning Models","metadata":{}},{"cell_type":"markdown","source":"## Objective 1: Determine Essential Physicochemical Properties Affecting Wine Quality","metadata":{}},{"cell_type":"markdown","source":"## Data Preparation and Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"Machine learning models typically require numerical input, so we need to convert categorical data to numerical form.\nTo address this, we can use one-hot encoding for the categorical variables. ","metadata":{}},{"cell_type":"markdown","source":"### One-Hot Encoding:\n\nOne-hot encoding was applied to the 'qualitytype' column","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import OneHotEncoder\n\n# One-hot encode the 'qualitytype' column\none_hot_encoder = OneHotEncoder(sparse=False, drop='first')\nencoded_qualitytype = one_hot_encoder.fit_transform(train[['qualitytype']])\nencoded_qualitytype_test = one_hot_encoder.transform(test[['qualitytype']])\n\n# Add one-hot encoded columns back to the dataframe and drop original 'qualitytype' column\nencoded_qualitytype_df = pd.DataFrame(encoded_qualitytype, columns=one_hot_encoder.get_feature_names_out(['qualitytype']))\nencoded_qualitytype_test_df = pd.DataFrame(encoded_qualitytype_test, columns=one_hot_encoder.get_feature_names_out(['qualitytype']))\n\ntrain = pd.concat([train.reset_index(drop=True), encoded_qualitytype_df], axis=1).drop(columns=['qualitytype'])\ntest = pd.concat([test.reset_index(drop=True), encoded_qualitytype_test_df], axis=1).drop(columns=['qualitytype'])\n","metadata":{"execution":{"iopub.status.busy":"2024-08-04T23:45:06.542163Z","iopub.execute_input":"2024-08-04T23:45:06.542550Z","iopub.status.idle":"2024-08-04T23:45:06.571176Z","shell.execute_reply.started":"2024-08-04T23:45:06.542522Z","shell.execute_reply":"2024-08-04T23:45:06.570129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The OneHotEncoder is used to transform the 'qualitytype' column into a one-hot encoded format.\nThe one-hot encoded columns are then concatenated back to the original training and testing DataFrames, and the original 'qualitytype' column is dropped.\nThis step converts the 'qualitytype' column into binary columns (one for each category), ensuring that the data is numerical. This transformation is crucial because machine learning models like Random Forest and XGBoost require numerical inputs.","metadata":{}},{"cell_type":"markdown","source":"## Model Training and Evaluation","metadata":{}},{"cell_type":"markdown","source":"### Random Forest Classifier\nWe trained a Random Forest Classifier to predict wine quality.","metadata":{}},{"cell_type":"code","source":"# Train Random Forest Classifier\nrf_classifier = RandomForestClassifier(n_estimators=500, random_state=123)\nrf_classifier.fit(train.drop(columns=['quality', 'type_bin']), train['quality'])\n\n# Make predictions\npredictions_rf = rf_classifier.predict(test.drop(columns=['quality', 'type_bin']))\n\n# Evaluate model\nconf_matrix_rf = confusion_matrix(test['quality'], predictions_rf)\nprint(\"Confusion Matrix:\\n\", conf_matrix_rf)\n\n# Feature importance\nimportance_matrix = rf_classifier.feature_importances_\nfeatures = train.drop(columns=['quality', 'type_bin']).columns\nimportance_df = pd.DataFrame({'Feature': features, 'Importance': importance_matrix}).sort_values(by='Importance', ascending=False)\n\n# Visualize feature importance\nplt.figure(figsize=(10, 8))\nsns.barplot(x='Importance', y='Feature', data=importance_df, palette='viridis')\nplt.title('Feature Importance from Random Forest Classifier')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-05T00:04:05.183418Z","iopub.execute_input":"2024-08-05T00:04:05.183792Z","iopub.status.idle":"2024-08-05T00:04:11.259504Z","shell.execute_reply.started":"2024-08-05T00:04:05.183764Z","shell.execute_reply":"2024-08-05T00:04:11.258398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* The confusion matrix provides insight into how well the model classifies different quality categories. The matrix indicates that the model performs well in distinguishing between most quality categories but struggles with some, especially for lower quality categories.\n    - The model performs well for class 3 and class 4, showing a high number of correct predictions.\n    - Class 1 is poorly predicted, suggesting that the model struggles to identify this class.\n    - Class 2 also has a high number of misclassifications, primarily being mistaken for class 3.\n    - Class 5 has significant confusion with class 4.\n    - Classes 6 and 7 have limited data, which might contribute to their poor performance.\n\n* The Feature importance analysis helps understand which features are most influential\n    - **Top Features:** Features with the highest importance scores are most influential in predicting wine quality. These features should be carefully analyzed for their impact on wine characteristics.\n    - **Impact on Wine Quality:** Understanding feature importance helps identify which factors most affect wine quality, guiding improvements in wine production and quality control.","metadata":{}},{"cell_type":"markdown","source":"## Objective 2: Build Regression Models for Predicting Wine Quality","metadata":{}},{"cell_type":"markdown","source":"###  Random Forest Regressor\n\nFor predicting the exact quality score, we employed a Random Forest Regressor. The RMSE value helps evaluate how well the model predicts actual quality scores.","metadata":{}},{"cell_type":"code","source":"# Random Forest Regression \n# Ensure 'quality' column is numeric\nwine_data['quality'] = wine_data['quality'].astype(float)\n\n# One-hot encode the 'qualitytype' column\none_hot_encoder = OneHotEncoder(sparse=False, drop='first')\nencoded_qualitytype = one_hot_encoder.fit_transform(wine_data[['qualitytype']])\n\n# Add one-hot encoded columns back to the dataframe and drop original 'qualitytype' column\nencoded_qualitytype_df = pd.DataFrame(encoded_qualitytype, columns=one_hot_encoder.get_feature_names_out(['qualitytype']))\nwine_data = pd.concat([wine_data.reset_index(drop=True), encoded_qualitytype_df], axis=1).drop(columns=['qualitytype'])\n\n# Split the data into training and testing sets\ntrain_set, test_set = train_test_split(wine_data, test_size=0.3, random_state=123)\n\n# Train Random Forest Regressor\nrf_regressor = RandomForestRegressor(n_estimators=500, random_state=123)\nrf_regressor.fit(train_set.drop(columns=['quality', 'type_bin']), train_set['quality'])\n\n# Make predictions\npredictions_rf_reg = rf_regressor.predict(test_set.drop(columns=['quality', 'type_bin']))\n\n# Calculate RMSE\nrmse_rf_reg = np.sqrt(mean_squared_error(test_set['quality'], predictions_rf_reg))\nprint(\"Random Forest Regression RMSE:\", rmse_rf_reg)\n\n# Feature importance\nimportance_matrix = rf_regressor.feature_importances_\nfeatures = train_set.drop(columns=['quality', 'type_bin']).columns\nimportance_df = pd.DataFrame({'Feature': features, 'Importance': importance_matrix}).sort_values(by='Importance', ascending=False)\n\n# Visualize feature importance\nplt.figure(figsize=(10, 8))\nsns.barplot(x='Importance', y='Feature', data=importance_df, palette='viridis')\nplt.title('Feature Importance from Random Forest Regressor')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-04T23:45:20.711496Z","iopub.execute_input":"2024-08-04T23:45:20.711904Z","iopub.status.idle":"2024-08-04T23:45:34.676127Z","shell.execute_reply.started":"2024-08-04T23:45:20.711871Z","shell.execute_reply":"2024-08-04T23:45:34.674788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Root Mean Squared Error (RMSE):**\n\n**RMSE = 0.4165:** This metric measures the average error between the predicted and actual wine quality scores. A lower RMSE indicates better predictive accuracy, which can be critical for quality assessment and improvement.","metadata":{}},{"cell_type":"markdown","source":"### XGBoost Regression\nWe also used XGBoost for regression","metadata":{}},{"cell_type":"code","source":"# XGBoost Regression (Objective 2)\n# Define predictors and responses\ntrain_x = train_set.drop(columns=['quality', 'type_bin'])\ntrain_y = train_set['quality']\ntest_x = test_set.drop(columns=['quality', 'type_bin'])\ntest_y = test_set['quality']\n\n# Create DMatrix objects\ntrain_xgb = xgb.DMatrix(data=train_x, label=train_y)\ntest_xgb = xgb.DMatrix(data=test_x, label=test_y)\n\n# Train XGBoost model\nxgb_model = xgb.train(params={'max_depth': 3, 'objective': 'reg:squarederror'}, dtrain=train_xgb, num_boost_round=100)\n\n# Make predictions\npred_xgb = xgb_model.predict(test_xgb)\n\n# Calculate RMSE\nrmse_xgb = np.sqrt(mean_squared_error(test_y, pred_xgb))\nprint(\"XGBoost Regression RMSE:\", rmse_xgb)\n\n# Convert predictions and actual values to categories\npred_xgb_cat = pd.cut(pred_xgb, bins=[0, 4.5, 6.5, 10], labels=['Low', 'Medium', 'High'])\ntest_y_cat = pd.cut(test_y, bins=[0, 4.5, 6.5, 10], labels=['Low', 'Medium', 'High'])\n\n# Create confusion matrix\nconf_matrix_xgb = confusion_matrix(test_y_cat, pred_xgb_cat)\nprint(\"XGBoost Confusion Matrix:\\n\", conf_matrix_xgb)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-04T23:45:51.087124Z","iopub.execute_input":"2024-08-04T23:45:51.087877Z","iopub.status.idle":"2024-08-04T23:45:51.228113Z","shell.execute_reply.started":"2024-08-04T23:45:51.087843Z","shell.execute_reply":"2024-08-04T23:45:51.227047Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**RMSE Comparison:**\n\n**RMSE = 0.4558:** XGBoost has a slightly higher RMSE compared to the Random Forest Regressor. This indicates that while XGBoost performs well, it may not be as accurate as Random Forest in predicting exact quality scores.\n\n**Confusion Matrix**\nWe also used XGBoost to create a confusion matrix by categorizing the predicted and actual values. The model performs well in predicting high quality (Class 3), but struggles more with low quality predictions. This suggests potential improvements in handling lower quality predictions.","metadata":{}},{"cell_type":"markdown","source":"### Decision Tree Classification\nWe trained a Decision Tree Classifier to see how well it performs. Decision Trees can be used to make interpretative decisions about the most influential features, which can guide quality control measures.","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.tree import export_text\n\n# Ensure 'quality' column is treated as a categorical variable\nwine_data['quality'] = wine_data['quality'].astype('category')\n\n# Split the data into training and testing sets\ntrain_set, test_set = train_test_split(wine_data, test_size=0.3, random_state=123)\n\n# Train the Decision Tree Classifier\ndt_classifier = DecisionTreeClassifier(random_state=123)\ndt_classifier.fit(train_set.drop(columns=['quality', 'type_bin']), train_set['quality'])\n\n# Get feature names and class names\nfeature_names = train_set.drop(columns=['quality', 'type_bin']).columns\nclass_names = [str(cat) for cat in train_set['quality'].cat.categories]\n\n# Get feature importances\nimportances = dt_classifier.feature_importances_\nindices = np.argsort(importances)[::-1]\n\n# Plot feature importances\nplt.figure(figsize=(12, 8))\nplt.title('Feature Importances')\nplt.bar(range(train_set.drop(columns=['quality', 'type_bin']).shape[1]), importances[indices], align='center')\nplt.xticks(range(train_set.drop(columns=['quality', 'type_bin']).shape[1]), np.array(feature_names)[indices], rotation=90)\nplt.xlim([-1, train_set.drop(columns=['quality', 'type_bin']).shape[1]])\nplt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-04T23:53:28.342242Z","iopub.execute_input":"2024-08-04T23:53:28.343425Z","iopub.status.idle":"2024-08-04T23:53:28.753314Z","shell.execute_reply.started":"2024-08-04T23:53:28.343380Z","shell.execute_reply":"2024-08-04T23:53:28.752089Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Feature Importance:** Similar to Random Forest, the Decision Tree provides insights into which features are most impactful. This helps in understanding the key factors affecting wine quality.","metadata":{}},{"cell_type":"markdown","source":"## Objective 3: Build Classification Models for Predicting Wine Type","metadata":{}},{"cell_type":"markdown","source":"### Support Vector Machine (SVM) Classification\nSVM was used for classification","metadata":{}},{"cell_type":"code","source":"# Support Vector Machine (SVM) Classification\n\nfrom sklearn.svm import SVC\n\n# Train SVM Classifier\nsvm_classifier = SVC(kernel='linear', random_state=123)\nsvm_classifier.fit(train.drop(columns=['type_bin', 'quality']), train['type_bin'])\n\n# Make predictions\npredictions_svm = svm_classifier.predict(test.drop(columns=['type_bin', 'quality']))\n\n# Evaluate model\nconf_matrix_svm = confusion_matrix(test['type_bin'], predictions_svm)\naccuracy_svm = accuracy_score(test['type_bin'], predictions_svm)\nprecision_svm = precision_score(test['type_bin'], predictions_svm)\nrecall_svm = recall_score(test['type_bin'], predictions_svm)\nf1_svm = f1_score(test['type_bin'], predictions_svm)\n\nprint(\"SVM Confusion Matrix:\\n\", conf_matrix_svm)\nprint(f\"SVM Accuracy: {accuracy_svm}, Precision: {precision_svm}, Recall: {recall_svm}, F1 Score: {f1_svm}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-04T23:55:52.489490Z","iopub.execute_input":"2024-08-04T23:55:52.490243Z","iopub.status.idle":"2024-08-04T23:55:53.810325Z","shell.execute_reply.started":"2024-08-04T23:55:52.490208Z","shell.execute_reply":"2024-08-04T23:55:53.809188Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Metrics:**\n\nAccuracy: 0.9841\nPrecision: 0.9792\nRecall: 0.9573\nF1 Score: 0.9681\n\n\n**Analysis**\nSVM demonstrates high accuracy, precision, recall, and F1 score, indicating its effectiveness in classifying wine types accurately.","metadata":{}},{"cell_type":"markdown","source":"### k-Nearest Neighbors (k-NN) Classification\nFinally, we used k-NN","metadata":{}},{"cell_type":"code","source":"# k-Nearest Neighbors (k-NN) Classification\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Train k-NN Classifier\nknn_classifier = KNeighborsClassifier(n_neighbors=5)\nknn_classifier.fit(train.drop(columns=['type_bin', 'quality']), train['type_bin'])\n\n# Make predictions\npredictions_knn = knn_classifier.predict(test.drop(columns=['type_bin', 'quality']))\n\n# Evaluate model\nconf_matrix_knn = confusion_matrix(test['type_bin'], predictions_knn)\naccuracy_knn = accuracy_score(test['type_bin'], predictions_knn)\nprecision_knn = precision_score(test['type_bin'], predictions_knn)\nrecall_knn = recall_score(test['type_bin'], predictions_knn)\nf1_knn = f1_score(test['type_bin'], predictions_knn)\n\nprint(\"k-NN Confusion Matrix:\\n\", conf_matrix_knn)\nprint(f\"k-NN Accuracy: {accuracy_knn}, Precision: {precision_knn}, Recall: {recall_knn}, F1 Score: {f1_knn}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-04T23:56:30.481002Z","iopub.execute_input":"2024-08-04T23:56:30.481422Z","iopub.status.idle":"2024-08-04T23:56:30.647342Z","shell.execute_reply.started":"2024-08-04T23:56:30.481391Z","shell.execute_reply":"2024-08-04T23:56:30.646181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Metrics:**\n\nAccuracy: 0.9390\nPrecision: 0.9307\nRecall: 0.8191\nF1 Score: 0.8714\n\n\n**Analysis**\nk-NN is accurate but less precise than SVM, struggling with false positives. It is useful but may not be as effective in scenarios requiring high precision.","metadata":{}},{"cell_type":"markdown","source":"### Model Comparison\nFinally, we compared the performance metrics of SVM and k-NN","metadata":{}},{"cell_type":"code","source":"# Create a DataFrame with performance metrics for SVM and k-NN\nmodel_comparison_classification = pd.DataFrame({\n    'Model': ['SVM', 'k-NN'],\n    'Accuracy': [accuracy_svm, accuracy_knn],\n    'Precision': [precision_svm, precision_knn],\n    'Recall': [recall_svm, recall_knn],\n    'F1 Score': [f1_svm, f1_knn]\n})\n\n# Display the comparison table\nprint(model_comparison_classification)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-04T23:56:40.719236Z","iopub.execute_input":"2024-08-04T23:56:40.720006Z","iopub.status.idle":"2024-08-04T23:56:40.729829Z","shell.execute_reply.started":"2024-08-04T23:56:40.719970Z","shell.execute_reply":"2024-08-04T23:56:40.728767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"SVM vs k-NN: SVM outperforms k-NN across all metrics, making it the preferred model for classification tasks requiring high accuracy and precision.","metadata":{}},{"cell_type":"markdown","source":"### **Conclusion**\n\n#### **Summary of Findings**\n\n1. **Exploratory Data Analysis (EDA)**\n   - The EDA reveals notable differences between red and white wines regarding acidity, alcohol content, and residual sugar. Red wines generally exhibit higher acidity and lower residual sugar, contributing to their bold flavor, while white wines have higher alcohol content and greater sweetness variability.\n   - The quality distribution shows that red wines are predominantly mid-range in quality, whereas white wines span a broader quality range. Specific features such as citric acid, as well as relationships observed through scatter plots and correlation heatmaps, provide valuable insights into how these attributes influence wine quality.\n   - These findings emphasize the importance of key features in determining wine quality and can guide both descriptive and predictive analyses for better understanding and improving wine characteristics.\n\n\n2. **Machine Learning Models**\n   - **Random Forest Classifier**\n     - **Confusion Matrix Insights:**\n       - Struggles with very low-quality wines (Class 1) and occasionally misclassifies lower-quality wines (Class 2).\n       - Performs well with mid-range (Class 3) and higher-quality wines (Class 4), but shows some confusion between high-quality (Class 4) and very high-quality wines (Class 5).\n       - Limited data and poor performance with Class 6 and Class 7 may reflect challenges in distinguishing the highest quality wines.\n     - **Feature Importance:** Identifies crucial features for predicting wine quality, helping to focus on factors that significantly impact wine characteristics.\n\n\n   - **Random Forest Regressor**\n     - **RMSE: 0.4165** indicates good accuracy in predicting exact quality scores, suggesting that the model effectively captures the quality nuances of wines.\n\n\n   - **XGBoost Regression**\n     - **RMSE: 0.4558** shows slightly less accuracy compared to Random Forest but remains a strong model. Further tuning could enhance performance.\n\n\n   - **Decision Tree Classifier**\n     - Provides intuitive insights into feature importance and decision-making processes, aiding in the interpretation of how different features affect wine quality.\n\n\n   - **Support Vector Machine (SVM) Classification**\n     - **Metrics:**\n       - Accuracy: 0.9841\n       - Precision: 0.9792\n       - Recall: 0.9573\n       - F1 Score: 0.9681\n     - SVM demonstrates high performance across all metrics, making it effective at distinguishing between different wine types, particularly for high and low-quality wines.\n\n\n   - **k-Nearest Neighbors (k-NN) Classification**\n     - **Metrics:**\n       - Accuracy: 0.9390\n       - Precision: 0.9307\n       - Recall: 0.8191\n       - F1 Score: 0.8714\n     - While accurate, k-NN is less precise than SVM and may face challenges with false positives. It remains useful but might not be ideal for scenarios demanding high precision.\n\n\n   - **Predictive Performance:** Random Forest and SVM are recommended for high accuracy and precision in predicting wine quality. XGBoost is also effective but slightly less accurate than Random Forest.\n   - **Feature Analysis:** Key features impacting wine quality have been identified, guiding improvement efforts and focusing on influential attributes.\n   - **Class-Specific Insights:** Models highlight difficulties in predicting extreme quality classes, suggesting a need for more data or improved features to differentiate these classes better.\n   - **Model Suitability:** SVM is preferred for high accuracy and precision tasks, while Random Forest and Decision Trees offer valuable interpretability. k-NN is useful but may not be the best choice for high-precision needs.\n\n\n#### **Practical Implications**\n\n- **Wine Production and Quality Control:** Insights from the analysis can guide producers in optimizing their winemaking processes by focusing on key attributes such as fixed acidity, residual sugar, and alcohol content. Adjustments in these areas can enhance wine quality.\n- **Feature Importance for Quality Improvement:** Understanding the most impactful attributes enables targeted efforts in improving wine quality, such as adjusting acidity and sugar levels to meet desired quality standards.\n- **Model Selection for Quality Prediction:** Random Forest and XGBoost models are recommended for predicting wine quality due to their accuracy. These models can support quality assessment and improvement strategies.\n\n#### **Suggested Future Work**\n\n- **Deeper Feature Analysis:** Future research could involve a more detailed analysis of specific features and their interactions to refine the understanding of their impact on wine quality.\n- **Additional Models and Techniques:** Exploring advanced machine learning techniques and models may further enhance prediction accuracy and provide additional insights.\n- **Application in the Wine Industry:** Implementing these findings in practical settings, such as quality control and production optimization, can lead to improved wine quality and more targeted production practices.\n","metadata":{}},{"cell_type":"markdown","source":"# **References**\n\n1. Cortez, P., Cerdeira, A., Almeida, F., Matos, T., & Reis, J. (2009). Modeling wine preferences by data mining from physicochemical properties. *Decision Support Systems, 47*(4), 547-553. doi:10.1016/j.dss.2009.05.016\n2. Jackson, R. S. (2020). *Wine Science: Principles and Applications* (5th ed.). Academic Press.\n3. Kennedy, J. A. (2008). Grape and wine phenolics: Observations and recent findings. *Ciencia e Investigaci√≥n Agraria, 35*(3), 107-120. doi:10.4067/S0718-16202008000300001\n4. Rib√©reau-Gayon, P., Glories, Y., Maujean, A., & Dubourdieu, D. (2006). *Handbook of Enology, Volume 2: The Chemistry of Wine - Stabilization and Treatments* (2nd ed.). John Wiley & Sons, Ltd.\n5. Breiman, L. (2001). Random forests. *Machine Learning, 45*(1), 5-32. doi:10.1023/A:1010933404324\n6. Chen, T., & Guestrin, C. (2016). XGBoost: A scalable tree boosting system. In *Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining* (pp. 785-794). doi:10.1145/2939672.2939785\n7. Quinlan, J. R. (1986). Induction of decision trees. *Machine Learning, 1*(1), 81-106. doi:10.1023/A:1022643204877\n8. Cortes, C., & Vapnik, V. (1995). Support-vector networks. *Machine Learning, 20*(3), 273-297. doi:10.1007/BF00994018\n9. Cover, T., & Hart, P. (1967). Nearest neighbor pattern classification. *IEEE Transactions on Information Theory, 13*(1), 21-27. doi:10.1109/TIT.1967.1053964\n10. Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning: Data Mining, Inference, and Prediction* (2nd ed.). Springer. doi:10.1007/978-0-387-84858-7\n\n","metadata":{}}]}